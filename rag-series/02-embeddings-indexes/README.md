# Blog 2.2: Embeddings, Indexes & Retrieval Mechanics ğŸ“š

This directory contains comprehensive code examples that implement all concepts covered in **[Blog 2.2: Embeddings, Indexes & Retrieval Mechanics](https://medium.com/@sadikkhadeer/embeddings-indexes-retrieval-mechanics-7d1f189b91c2)** of the RAG Fundamentals series.

## ğŸ¯ What You'll Learn

This code demonstrates the core building blocks of RAG systems:

- **ğŸ—ºï¸ Embeddings**: How text becomes searchable vectors
- **ğŸ“ Similarity Metrics**: Cosine, dot product, and Euclidean distance  
- **âš¡ Indexing Structures**: FAISS for efficient similarity search
- **ğŸ—ƒï¸ Vector Databases**: Chroma and Qdrant with production features
- **ğŸ”„ Complete RAG Pipeline**: End-to-end retrieval system

## ğŸ“ File Structure

```
examples/
â”œâ”€â”€ 01_embeddings_fundamentals.py    # Core embedding concepts and visualization
â”œâ”€â”€ 02_similarity_metrics.py         # All similarity measures with comparisons
â”œâ”€â”€ 03_indexing_structures.py        # FAISS implementations and benchmarking
â”œâ”€â”€ 04_vector_databases.py          # Chroma & Qdrant with advanced features
â””â”€â”€ 05_complete_rag_pipeline.py     # End-to-end RAG system integration

requirements.txt                     # All dependencies needed
```

## ğŸš€ Quick Start

1. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Explore individual concepts**:
   ```bash
   # Start with embeddings fundamentals
   python examples/01_embeddings_fundamentals.py
   
   # Then explore similarity metrics
   python examples/02_similarity_metrics.py
   
   # And so on...
   ```

## ğŸ” Key Concepts Implemented

### 1ï¸âƒ£ Embeddings: The Meaning Map
- **What**: Converting text to numerical vectors that capture semantic meaning
- **How**: Using SentenceTransformers with multiple model comparisons
- **Code**: `01_embeddings_fundamentals.py`

**Example from code**:
```python
model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = model.encode(["maternity leave policy", "pizza recipe"])
# Results in 384-dimensional vectors that capture semantic meaning
```

### 2ï¸âƒ£ Similarity Metrics: Finding What's Related  
- **Cosine Similarity**: Measures angle between vectors (most common)
- **Dot Product**: Raw similarity without normalization
- **Euclidean Distance**: Geometric distance in vector space
- **Code**: `02_similarity_metrics.py`

**Example from code**:
```python
# All three metrics demonstrate different aspects of similarity
cosine_sim = cosine_similarity(query_emb, doc_embs)
dot_product = np.dot(query_emb[0], doc_embs.T) 
euclidean_dist = np.linalg.norm(doc_embs - query_emb, axis=1)
```

### 3ï¸âƒ£ Indexing Structures: Speed Through Smart Organization
- **FAISS Flat**: Exact search for smaller datasets
- **FAISS HNSW**: Approximate Nearest Neighbor for speed
- **FAISS IVF**: Inverted file structure for large datasets  
- **FAISS PQ**: Product quantization for memory efficiency
- **Code**: `03_indexing_structures.py`

**Example from code**:
```python
# Different indexes for different use cases
flat_index = faiss.IndexFlatIP(dimension)      # Exact search
hnsw_index = faiss.IndexHNSWFlat(dimension, 32) # Fast approximate
ivf_index = faiss.IndexIVFFlat(quantizer, dimension, nlist) # Scalable
```

### 4ï¸âƒ£ Vector Databases: Production-Ready Storage
- **Chroma**: Local-first with easy setup
- **Qdrant**: Production-scale with advanced filtering
- **Features**: Metadata filtering, persistence, CRUD operations
- **Code**: `04_vector_databases.py`

**Example from code**:
```python
# Production features like metadata filtering
collection.query(
    query_texts=["leave policy"],
    where={"department": "hr", "active": True},
    n_results=10
)
```

### 5ï¸âƒ£ Complete RAG Pipeline: Putting It All Together
- **Document Processing**: Chunking and embedding
- **Hybrid Search**: Vector + keyword search combination  
- **Retrieval**: Context-aware document selection
- **Production Patterns**: Error handling, monitoring, caching
- **Code**: `05_complete_rag_pipeline.py`

**Example from code**:
```python
class RAGRetriever:
    def hybrid_search(self, query, alpha=0.7):
        # Combine vector and keyword search
        vector_scores = self._vector_search(query)
        bm25_scores = self._keyword_search(query)
        return self._merge_results(vector_scores, bm25_scores, alpha)
```

## ğŸ§ª Testing & Validation

Each example script includes built-in validation that demonstrates blog concepts working correctly:

- âœ… **Embeddings capture semantic meaning** (similar concepts cluster together)
- âœ… **Similarity metrics rank correctly** (related docs score higher)
- âœ… **FAISS indexing enables fast search** (sub-second retrieval)
- âœ… **Vector databases support production features** (metadata, persistence)  
- âœ… **Complete pipeline handles real queries** (hybrid search works)

## ğŸ“Š Performance Notes

From the code benchmarks:
- **FAISS Flat**: 100% accuracy, slower for large datasets
- **FAISS HNSW**: ~95% accuracy, 10-100x faster
- **FAISS IVF**: 90-95% accuracy, best for millions of vectors
- **Vector DBs**: Add 10-50ms overhead but provide production features

## ğŸ”— Connection to Blog Series

This code directly implements examples from:
- **[Blog 2.1](https://medium.com/@sadikkhadeer/rag-fundamentals-solving-the-knowledge-problem-7d4f6b0eda3a)**: RAG Fundamentals (foundation concepts)
- **[Blog 2.2](https://medium.com/@sadikkhadeer/embeddings-indexes-retrieval-mechanics-7d1f189b91c2)**: Embeddings & Indexing (this implementation) 
- **Blog 2.3**: Advanced RAG Patterns (coming next)

## ğŸ’¡ Next Steps

After mastering these concepts:
1. **Explore the full example files** for comprehensive implementations
2. **Experiment with different embedding models** in `01_embeddings_fundamentals.py`  
3. **Benchmark indexing structures** with your own data using `03_indexing_structures.py`
4. **Build your own RAG system** using patterns from `05_complete_rag_pipeline.py`

## ğŸ¤ Educational Approach

Each file follows the blog's learner-first methodology:
- **Real-world examples** (HR policies, recipes)
- **Progressive complexity** (simple to advanced)
- **Practical insights** (when to use what)  
- **Production patterns** (error handling, monitoring)

---

*This code accompanies the [Data Science Series Blog](https://medium.com/@sadikkhadeer/data-science-series-complete-learning-path-updated-weekly-83611dea41fb). For the complete learning experience, read the [blog post](https://medium.com/@sadikkhadeer/embeddings-indexes-retrieval-mechanics-7d1f189b91c2) alongside running these examples.*