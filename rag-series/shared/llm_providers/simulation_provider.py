"""
Simulation Provider for RAG Systems
Blog 2.3: From Retrieval to Answers - The Full RAG Pipeline

A simulation provider that generates realistic responses without calling actual APIs.
Useful for testing, development, and demonstration when API keys are not available.
"""

import time
import asyncio
import random
from typing import Optional

from .base_llm import BaseLLMProvider, LLMConfig, LLMResponse


class SimulationProvider(BaseLLMProvider):
    """
    Simulation provider that generates realistic responses for RAG systems.
    
    Features:
    - No API calls required
    - Realistic response patterns
    - Proper usage tracking simulation
    - Useful for testing and development
    """
    
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.provider_name = "simulation"
    
    async def generate(self, prompt: str, system_prompt: Optional[str] = None, 
                      context: Optional[str] = None) -> LLMResponse:
        """Generate simulated response."""
        # Simulate API call delay
        await asyncio.sleep(random.uniform(0.5, 2.0))
        
        return self._generate_response(prompt, system_prompt, context)
    
    def generate_sync(self, prompt: str, system_prompt: Optional[str] = None,
                     context: Optional[str] = None) -> LLMResponse:
        """Synchronous version of generate."""
        # Simulate API call delay
        time.sleep(random.uniform(0.5, 2.0))
        
        return self._generate_response(prompt, system_prompt, context)
    
    def _generate_response(self, prompt: str, system_prompt: Optional[str] = None,
                          context: Optional[str] = None) -> LLMResponse:
        """Generate a simulated response based on the prompt and context."""
        
        # Analyze prompt to generate appropriate response
        prompt_lower = prompt.lower()
        
        if context:
            # RAG-style response using context
            if "what is" in prompt_lower or "define" in prompt_lower:
                content = f"Based on the provided context: {context[:200]}... The answer to your question is that {prompt.split()[-1]} is a concept that involves multiple aspects as described in the context. This is a simulated response that demonstrates how RAG systems work by incorporating retrieved information."
            elif "when" in prompt_lower or "date" in prompt_lower:
                content = f"According to the provided context, the timing/date information shows that this occurred as described in the retrieved documents. The context indicates specific temporal information relevant to your query about {prompt.split()[-1]}."
            elif "how" in prompt_lower:
                content = f"The context provides a step-by-step explanation: Based on the retrieved information, the process involves multiple stages as outlined in the source material. This demonstrates how RAG systems can provide detailed procedural knowledge."
            else:
                content = f"Drawing from the provided context, I can see that your question about '{prompt}' relates to information contained in the retrieved documents. The context shows relevant details that help answer your query comprehensively."
        else:
            # Standard response without context
            if "hello" in prompt_lower or "hi" in prompt_lower:
                content = "Hello! I'm a simulated LLM provider for testing RAG systems. How can I help you today?"
            elif "test" in prompt_lower:
                content = "This is a test response from the simulation provider. Everything is working correctly!"
            elif "what is rag" in prompt_lower:
                content = "RAG (Retrieval-Augmented Generation) is a technique that combines information retrieval with language generation to provide more accurate and factual responses."
            else:
                content = f"This is a simulated response to your query: '{prompt}'. In a real RAG system, this would be generated by an actual language model using retrieved context from your knowledge base."
        
        # Simulate realistic usage statistics
        prompt_tokens = len(prompt.split()) * 1.3  # Rough token estimation
        context_tokens = len(context.split()) * 1.3 if context else 0
        total_prompt_tokens = int(prompt_tokens + context_tokens)
        completion_tokens = len(content.split()) * 1.3
        total_tokens = int(total_prompt_tokens + completion_tokens)
        
        usage = {
            "prompt_tokens": total_prompt_tokens,
            "completion_tokens": int(completion_tokens),
            "total_tokens": total_tokens,
            "response_time": random.uniform(0.5, 2.0),
            "cost_estimate": total_tokens * 0.00001  # Simulated cost
        }
        
        return LLMResponse(
            content=content,
            model=self.config.model,
            provider=self.provider_name,
            usage=usage,
            metadata={
                "finish_reason": "stop",
                "response_id": f"sim_{int(time.time())}_{random.randint(1000, 9999)}",
                "context_provided": context is not None,
                "context_length": len(context) if context else 0,
                "simulation": True
            }
        )
    
    def is_available(self) -> bool:
        """Simulation provider is always available."""
        return True
    
    def get_token_count(self, text: str) -> int:
        """Estimate token count for simulation."""
        return int(len(text.split()) * 1.3)


def get_simulation_provider(model: str = "gpt-4-simulation") -> SimulationProvider:
    """
    Create a simulation provider for testing RAG systems.
    
    Args:
        model: Model name to simulate
        
    Returns:
        Configured simulation provider
    """
    config = LLMConfig(
        model=model,
        max_tokens=1500,
        temperature=0.1,
        context_window=8192  # Simulated context window
    )
    
    return SimulationProvider(config)


async def demo_simulation_rag():
    """Demonstrate simulation provider with RAG context."""
    print("=== Simulation RAG Provider Demo ===\n")
    
    provider = get_simulation_provider()
    
    print(f"âœ… Simulation Provider initialized with model: {provider.config.model}")
    print(f"ðŸ“Š Provider available: {provider.is_available()}")
    
    # Test with RAG context
    context = """
    Machine learning is a subset of artificial intelligence that focuses on algorithms
    that can learn from and make predictions on data. It was first coined as a term
    by Arthur Samuel in 1959. Common types include supervised learning, unsupervised
    learning, and reinforcement learning.
    """
    
    print("\n1. RAG-style Query with Context:")
    response = await provider.generate(
        prompt="What is machine learning and when was the term first used?",
        context=context
    )
    
    print(f"Response: {response.content}")
    print(f"Usage: {response.usage}")
    print(f"Cost estimate: ${response.usage.get('cost_estimate', 0):.6f}")
    
    print("\n2. Simple Query without Context:")
    response2 = await provider.generate("Hello, how are you?")
    
    print(f"Response: {response2.content}")
    
    print("\nâœ… Simulation RAG Provider demo completed successfully!")


if __name__ == "__main__":
    asyncio.run(demo_simulation_rag())